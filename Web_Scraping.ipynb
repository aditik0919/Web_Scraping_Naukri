{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4 pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9dl95XgTwhM",
        "outputId": "1386a42c-2cec-4427-9ed2-e73a2dca8ad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# URL for Naukri.com search results (modify the query as needed)\n",
        "url = \"https://www.naukri.com/data-scientist-jobs?k=data%20scientist&experience=0&job_posted=last_7_days\"\n",
        "\n",
        "# Set headers to mimic a browser request\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "}\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(url, headers=headers)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Extract job postings data\n",
        "    jobs = []\n",
        "    job_cards = soup.find_all('article', class_='jobTuple bgWhite br4 mb-8')\n",
        "\n",
        "    for card in job_cards[:20]:  # Limit to 20 job postings\n",
        "        try:\n",
        "            title = card.find('a', class_='title').text.strip()\n",
        "            company = card.find('a', class_='subTitle').text.strip()\n",
        "            location = card.find('li', class_='fleft grey-text br2 placeHolderLi location').text.strip()\n",
        "            date_posted = card.find('div', class_='type br2 fleft grey').text.strip()\n",
        "            description = card.find('div', class_='ellipsis job-description').text.strip()\n",
        "            application_link = card.find('a', class_='title')['href']\n",
        "\n",
        "            jobs.append({\n",
        "                \"Job Title\": title,\n",
        "                \"Company Name\": company,\n",
        "                \"Location\": location,\n",
        "                \"Date Posted\": date_posted,\n",
        "                \"Job Description\": description,\n",
        "                \"Application Link\": application_link\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting a job card: {e}\")\n",
        "\n",
        "    # Save the extracted data to an Excel file\n",
        "    output_file = \"AditiKolhe_Naukri_Output.xlsx\"\n",
        "    df = pd.DataFrame(jobs)\n",
        "    df.to_excel(output_file, index=False)\n",
        "\n",
        "    print(f\"Scraping complete. Data saved to {output_file}\")\n",
        "else:\n",
        "    print(\"Failed to retrieve job postings. Please check the URL or your internet connection.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjebB1V2T2AM",
        "outputId": "eca0a227-edae-432e-88e4-6469a9e52614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping complete. Data saved to AditiKolhe_Naukri_Output.xlsx\n"
          ]
        }
      ]
    }
  ]
}